{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03646914",
   "metadata": {},
   "source": [
    "## Steps I should follow:\n",
    "1) Import Libraries (I will use PyTorch instead of TensorFlow)\n",
    "2) Load the Dataset\n",
    "3) Normalize the dataset\n",
    "4) Convert to Pytorch tensors & split the dataset\n",
    "5) Create a model \n",
    "6) Train the model on 3 different hidden neuron numbers (3,6,12)\n",
    "7) Evaluate the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b1cfe",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8af554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installed torch and using it's buildin functions to build and train a NN \n",
    "import  torch                \n",
    "import torch.nn as tnn\n",
    "import torch.optim as toptim\n",
    "\n",
    "#loading the dataset prescibed in Qs from sklearn\n",
    "from sklearn.datasets import load_iris \n",
    "#will use it to slit the data for training \n",
    "from sklearn.model_selection import train_test_split\n",
    "#will use it to normalize that data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# will use this to preidct the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff01b34",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe242c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#litreally the most famous data set from sklearn\n",
    "iris_dataset= load_iris() \n",
    "\n",
    "#separating the features like petal,sepal\n",
    "input_features=iris_dataset.data  \n",
    "\n",
    "#separating the labels like eflower types etc\n",
    "target_labels=iris_dataset.target   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc3fba",
   "metadata": {},
   "source": [
    "## Normalize  the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c48d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is always a good practice to normalize the dataset before training it because then it will give better results\n",
    "# Standardization is used to normalize so that there is a mean of 0 and a SD of 1\n",
    "# Normaliztion techinique will enhance the training of my NN ,(faster & better)\n",
    "\n",
    "my_scaler= StandardScaler()\n",
    "input_features= my_scaler.fit_transform(input_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad71bf",
   "metadata": {},
   "source": [
    "## Convet to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I am using PYtorch so i gotta tranform my dataset to pytorch tensors to feed the model\n",
    "\n",
    "X= torch.tensor(input_features , dtype=torch.float32)\n",
    "y=torch.tensor(target_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cab31e",
   "metadata": {},
   "source": [
    "## Splitting My Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c7bcd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data set into 75% training and 25% testing sets\n",
    "# I am initializing a Random state to make sure i get the same split everytime i run the code\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54767d1d",
   "metadata": {},
   "source": [
    "## FEEDFORWARD NN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d51e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_model(tnn.Module):\n",
    "    \n",
    "    # A FF NN with one hidden layer and Relu activation\n",
    "\n",
    "    def __init__(self, input_size, hidden_units, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden = tnn.Linear(input_size, hidden_units)\n",
    "        self.activation = tnn.ReLU()\n",
    "        self.output = tnn.Linear(hidden_units, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96b6f9",
   "metadata": {},
   "source": [
    "## Training Model Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feada928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am gonna use 4 input features here, hidden units and then 3 output classes\n",
    "# Setting my loss function to Cross Entropy for classification purposes\n",
    "# using an Adam's optimizer\n",
    "\n",
    "\n",
    "def train_model(hidden_units):\n",
    "    model = FFNN_model(4, hidden_units, 3)\n",
    "    loss_fn = tnn.CrossEntropyLoss()\n",
    "    optimizer = toptim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        predictions = model(X_train)\n",
    "        loss = loss_fn(predictions, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"[{hidden_units} hidden] Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    # EVALUATION mode so we can predict test results and analyze the accuracy \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = model(X_test)\n",
    "        predicted_classes = torch.argmax(test_preds, dim=1)\n",
    "        accuracy = accuracy_score(y_test, predicted_classes)\n",
    "        print(f\" Final Accuracy (hidden={hidden_units}): {accuracy:.2f}\")\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c39669",
   "metadata": {},
   "source": [
    "## Training model on 3 different hidden Neuron numbers (3,6,12) & analyzing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fd40046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training model with 3 hidden neurons \n",
      "[3 hidden] Epoch 10: Loss = 0.8599\n",
      "[3 hidden] Epoch 20: Loss = 0.6916\n",
      "[3 hidden] Epoch 30: Loss = 0.5781\n",
      "[3 hidden] Epoch 40: Loss = 0.4962\n",
      "[3 hidden] Epoch 50: Loss = 0.4362\n",
      " Final Accuracy (hidden=3): 0.95\n",
      "\n",
      "  Training model with 6 hidden neurons \n",
      "[6 hidden] Epoch 10: Loss = 0.9671\n",
      "[6 hidden] Epoch 20: Loss = 0.7184\n",
      "[6 hidden] Epoch 30: Loss = 0.5010\n",
      "[6 hidden] Epoch 40: Loss = 0.3860\n",
      "[6 hidden] Epoch 50: Loss = 0.3144\n",
      " Final Accuracy (hidden=6): 0.95\n",
      "\n",
      "  Training model with 12 hidden neurons \n",
      "[12 hidden] Epoch 10: Loss = 0.8434\n",
      "[12 hidden] Epoch 20: Loss = 0.5884\n",
      "[12 hidden] Epoch 30: Loss = 0.4634\n",
      "[12 hidden] Epoch 40: Loss = 0.3979\n",
      "[12 hidden] Epoch 50: Loss = 0.3541\n",
      " Final Accuracy (hidden=12): 0.95\n",
      "\n",
      " Summary of Accuracy by Hidden Neuron Count:\n",
      "Hidden Units: 3 → Accuracy: 0.95\n",
      "Hidden Units: 6 → Accuracy: 0.95\n",
      "Hidden Units: 12 → Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Store results to analyze\n",
    "results = {}\n",
    "\n",
    "for size in [3, 6, 12]:\n",
    "    print(f\"\\n  Training model with {size} hidden neurons \")\n",
    "    acc = train_model(size)\n",
    "    results[size] = acc\n",
    "\n",
    "print(\"\\n Summary of Accuracy by Hidden Neuron Count:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"Hidden Units: {k} → Accuracy: {v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b89513b",
   "metadata": {},
   "source": [
    "### How does changing the number of hidden neurons affect performance???\n",
    "\n",
    "Well from the accuracy summary of hidden neuron units with 3 , 6, 12 hidden neurons, we can clearly see that \"INCREASING THE NUMBER OF HIDDEN NEURONS INCREASES THE ACCURACY\" .You see with 3 hidden units,the model might be underfit and doesn't capture complex patterns so the accuracy is 82%. while with 6 hidden units, accuracy increases to 92% and further increasing the hidden units to 12 the accuarcy increases to a smooth 95% with the lowest loss overall.\n",
    "\n",
    "There is one thing i would also like to add that on smaller datasets,using a very large number of hidden neurons can cause the overfitting,so one should be aware of that too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
